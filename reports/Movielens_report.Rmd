---
title: "MovieLens Project Report"
author: "Kevin McGowan"
date: "2024-11-25"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This MovieLens project report, part of the HarvardX: PH125.9x Data Science: Capstone course, explores the development of a machine learning algorithm to predict movie ratings—a critical step in creating personalized recommendation systems. These systems have become essential tools in the video streaming industry, where engaging viewers with tailored content enhances satisfaction, retention, and engagement duration. This report outlines the step-by-step methodology for building and validating a machine learning model to predict movie ratings:

- **Introduction**
- **Methodology**
  - Exploratory Data Analysis: Understanding the dataset’s structure, patterns, and distributions.
  - Feature Development: Creating meaningful variables that capture user preferences and movie attributes.
  - Modeling: Applying regression techniques to treat movie ratings as a continuous variable.
  - Validation: Testing the model’s accuracy on independent data using Root Mean Squared Error (RMSE) as the primary performance metric.
  - Prediction on Independent Data: Evaluating the model on a completely independent holdout set.
- **Results**
- **Discussion**

In August 2024, Forbes reported that Netflix, the global leader in video streaming, had 260.28 million subscribers worldwide, with the global video streaming industry valued at $544 billion annually (Durrani & Allen, 2024). Personalized recommendation systems are vital for platforms like Netflix to keep viewers engaged by presenting content aligned with user preferences. This report focuses on developing a robust predictive model that leverages historical user behavior and movie characteristics to forecast ratings. Accurate rating predictions serve as the foundation for crafting personalized recommendations, not only improving viewer satisfaction but also informing strategic content decisions.

Beyond entertainment, machine learning models like this have broad applications in other industries, such as forecasting property values in real estate or predicting high school graduation rates in education. By demonstrating the process of building and validating a predictive model, this report provides insights into how historical patterns can inform future behavior and decision-making.

## The Dataset
This predictive model considers two primary sources of information: the user and the movie. The user’s input is represented by their past ratings, providing insight into their preferences. The movie information includes attributes like genre and aggregated ratings from other users. Unlike human reviewers, the algorithm is constrained to these structured data points but compensates by processing immense volumes of information.

The data set used in this machine learning task is the 10 million MovieLens data set, which contains 10,000,054 ratings applied to 10,681 movies by 71,567 users (Harper & Konstan, 2015). These data are spread across two primary files—ratings.dat and movies.dat—that need to be joined for analysis. Each user has been assigned a unique ID to anonymize their identity, and only users who rated at least 20 movies were included in the dataset. No additional demographic or behavioral information about the users is provided.

The ratings are recorded on a 5-star scale, allowing for 0.5-star increments for a total of 11 possible rating levels between 0.5 and 5. The data set includes 18 distinct genres, such as Action, Comedy, Drama, and Western.
ly used in industries such as streaming platforms (e.g., Netflix, Hulu), e-commerce, and online marketplaces. Next the method section loads and explores the data set, builds performance features, and trains the machine learning algorithm on the 10 million MovieLens data set.

# Method/Analysis
A methods/analysis section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach

## Data Wrangling 
Before we can train movie rating prediction algorithm out of the 10mil dataset, the data set needs to be prepared for analysis. The steps in doing so are as follows: 

- Load Data: download, unzip, and combine ratings.dat and movies.dat;
- Explore Data: variables, dimensions, and insights
- Feature Development: descriptive statistics of users and movies
- Model Development: regression, training, cross validaition
- Prediction: Forecasting ratings on the trianing set.
 
To prepare the data for model training and evaluation, the dataset was randomly split into an edx training set (90% of the data) and a final holdout test set (10% of the data). The split ensures that the training set includes all user-movie combinations present in the holdout set, preserving the integrity of the evaluation. This robust dataset is well-suited for predicting movie ratings.

Before training the movie rating prediction algorithm, we prepare the 10-million MovieLens dataset. The dataset is downloaded, unzipped, and merged by joining the `ratings.dat` and `movies.dat` files on the `movieId` field. The merged dataset includes user ratings, movie information (titles and genres), and timestamps.

We then split the dataset into two subsets: an **edx training set** (90% of the data) and a **final holdout test set** (10% of the data). To ensure integrity during evaluation, we verify that all user-movie combinations in the holdout set are also present in the training set. Below is the code used for these steps:

## Load Data
```{r load packages, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}
# List of required packages
packages <- c("readr", "tinytex", "dplyr", "caret", "stringr", "tidyverse", "tidyr", "broom", "glmnet", "Matrix","coefplot")

# Check and install missing packages
for (pkg in packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
        install.packages(pkg)
    }
}

# Load the libraries
lapply(packages, library, character.only = TRUE)
```

With the required packages loaded—collections of pre-written code, functions, and documentation that extend R’s functionality—we can now download the datasets two parts, define the variable types, combine them, and load it into the environment.

```{r load and combine dataset, message=FALSE, warning=FALSE}
# Increase timeout for downloading large datasets
options(timeout = 120)

# Download and unzip the dataset
dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

# load the user ratings half of the data set
ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

# load the movie half of the data set
movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

# Read and process the ratings file by specifying variable types
ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

# Read and process the movies file
movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

# Merge ratings and movies datasets by specifying variable types
movielens <- left_join(ratings, movies, by = "movieId")

```

With the two datasets combined into one movielens dataset, the final validation must be removed from the dataset to create an independent test set for evaluating the quality of the machine learning model.

```{r split dataset into training and final holdout, warning=FALSE}
# Split the data into training (edx) and final holdout test sets
set.seed(1, sample.kind="Rounding") # Use "Rounding" for R 3.6 or later
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Ensure all userId and movieId in the holdout set are present in the training set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add back any rows excluded from the holdout set into the training set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

# Clean up unnecessary variables
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

The dataset is now loaded, combined, and split into two parts: the edx training set (90%) and the final holdout test set (10%). This split ensures that the training set contains all user-movie combinations present in the test set, maintaining the integrity of the evaluation process.

## Explore Data
Before delving into feature development, it is essential to explore the data set to gain insights into its structure and characteristics. This exploratory phase increases understanding of the data set and guides subsequent steps in feature engineering and modeling.

The MovieLens 10-million dataset, now called (`edx`), contains information about user-movie interactions, including ratings, timestamps, movie titles, movie ID, user ID, and genres. Each row represents a user's rating for a specific movie. As seen in the code output below, the data set includes:

- **9,000,055 rows**
- **6 columns**: `userId`, `movieId`, `rating`, `timestamp`, `title`, and `genres`
- **69,878 unique users** and **10,677 unique movies**

```{r dataset structure and size, echo=TRUE}
# Display the first few rows of the dataset
head(edx)

# Count the number of rows and columns (TRY NOT USING CAT BECAUSE I DON'T THINK IT'S NEEDED IN R MARK DOWN)
cat("Number of rows:", nrow(edx), "\n")
cat("Number of columns:", ncol(edx), "\n")
```

### Rating Distribution
The ratings range from 0.5 to 5, in increments of 0.5 stars. A look at the most common ratings reveals:

- 4 stars: Most frequent rating, with 2,588,430 occurrences.
- 3 stars: Second most frequent, with 2,121,240 occurrences.
- Other frequent ratings include 5, 3.5, and 2 stars.
	
```{r data-ratings, echo=TRUE}
# Count the top 5 most common ratings
most_common_ratings <- edx %>%
  group_by(rating) %>%               
  summarise(total_occurrence = n()) %>%  
  arrange(desc(total_occurrence)) %>%    
  head(5)

print(most_common_ratings)

# show all unique ratings from smallest to largest
sorted_ratings <- sort(unique(edx$rating))
sorted_ratings
```
This clustering of ratings around 4 stars suggests averages and standard deviations of ratings for movies could provide insight into movie ratings.

### Most Rated Movies
Certain movies receive significantly more ratings, potentially biasing the model. The top-rated movies include:

- Pulp Fiction (1994): 31,362 ratings
- Forrest Gump (1994): 31,079 ratings
- The Silence of the Lambs (1991): 30,382 ratings

```{r data movies, echo=TRUE}
# Identify the top-rated movies
count_ratings_by_movie <- edx %>%
  group_by(title) %>%
  summarise(rating_counts = n()) %>%
  arrange(desc(rating_counts))

print(count_ratings_by_movie, n = 10)
```


### Genre Distribution
The dataset spans a wide array of genres. The four most frequent genres are:

- Drama: Appears 3,910,127 times.
- Comedy: Appears 3,540,930 times.
- Thriller: Appears 2,325,899 times.
- Romance: Appears 1,712,100 times.
	
There are a few movies with no genre listed. This edge case could present a problem when it comes to 

```{r data-genres and ratings, echo=TRUE}
# Count how many movies belong to specific genres
all_genres <- edx %>%
  pull(genres) %>%
  str_split("\\|") %>%
  unlist() %>%
  table() %>%
  as.data.frame()

# Rename the columns for clarity
colnames(all_genres) <- c("Genre", "Count")

# Sort the genres by count in descending order
all_genres <- all_genres %>%
  arrange(desc(Count))

# Plot the genres as a bar plot
library(ggplot2)
ggplot(all_genres, aes(x = reorder(Genre, -Count), y = Count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Number of Ratings by Genre",
       x = "Genre",
       y = "Count of Ratings") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### Movie Rating Distribution
The dataset spans a an entire century of movies. The the 4 decades with the most movie ratings are:

- 90s
- 2000s
- 80s
- 70s

```{r ratings by decade, echo=TRUE}
# Create a new dataset for exploration
edx_exploration <- edx %>%
  mutate(
    movie_year = as.numeric(gsub("[^0-9]", "", str_extract(title, "\\((\\d{4})\\)"))),
    decade = paste0(floor(movie_year / 10) * 10, "s")
  )

# Count ratings by decade
ratings_by_decade <- edx_exploration %>%
  group_by(decade) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# Plot a bar chart of ratings by decade
ggplot(ratings_by_decade, aes(x = reorder(decade, -count), y = count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(
    title = "Number of Ratings by Decade",
    x = "Decade",
    y = "Number of Ratings"
  ) +
  theme_minimal()

```

### Missing Values
Looking at the data set again, the top 10 rows show we still have 6 columns and 0 NAs
	
```{r review dataset, echo=TRUE}
#review top 10 rows of the data set
head(edx)

# Count the total number of missing values in the edx dataset
total_missing <- sum(is.na(edx))
cat("Total Missing Values in edx:", total_missing, "\n")
```

### Key Findings

This exploratory analysis highlights:

- The skew in ratings distribution, with users preferring mid-to-high ratings.
- A subset of movies dominates the dataset in terms of user engagement.
- Drama and Comedy are the most represented genres, reflecting their popularity among users.
- There are ratings for movies from 1950 to 2000s, with the majority in the 90s, presenting an opportunity to look a ratings by decade.
- There are no NAs but some movies have no genres applied to them, representing an edge case that should be accounted for in feature development.

This understanding lays the groundwork for the next step: Feature Development. By leveraging the insights gained here, we will create meaningful features that capture user preferences, movie attributes, and other contextual factors.


## Feature Engineering

To build an effective movie rating prediction model, we must carefully construct features that capture meaningful patterns from the data. Feature engineering is the process of transforming raw data into informative variables, or “features,” that enhance the predictive power of a model. This step bridges the gap between exploratory data analysis and machine learning, enabling the model to make more accurate predictions.

In the MovieLens dataset, features need to encapsulate key aspects of both users and movies. For example:

- User-specific features: The number of ratings a user has given, their average rating tendencies, and how consistent or inconsistent their ratings are over time.
- Movie-specific features: A movie’s release year, how recently it was rated, the number of ratings it has received, and the overall sentiment reflected in its ratings.
- Temporal features: Variables that account for the time of rating, such as the year, month, or even the day of the week.

By deriving these features, we transform the raw data into a structured data set that highlights patterns related to user preferences and movie characteristics.

To ensure consistency and scalability, we use a single processing function called feature_engineering. This function applies transformations to the raw data set in a systematic way, creating all the necessary features for modeling. Below is the code for the feature_engineering function, which includes user-specific, movie-specific, and temporal features:

```{r Feature engineering,echo = TRUE}
feature_engineering <- function(data) {
  
  # Step 1: Extract movie year and other temporal features
  data <- data %>%
    mutate(
      rating_year = year(as_datetime(timestamp)),
      rating_month = month(as_datetime(timestamp)),
      rating_day_of_week = wday(as_datetime(timestamp), label = TRUE),
      movie_year = as.numeric(gsub("[^0-9]", "", str_extract(title, "\\((\\d{4})\\)"))),
      movie_age = rating_year - movie_year,
      decade = paste0(floor(movie_year / 10) * 10, "s") # Decade feature
    )
  
  # Step 2: User-specific features 
  user_features <- data %>%
    group_by(userId) %>%
    summarise(
      user_rating_count = n(),#count of ratings per user
      user_avg_rating = mean(rating, na.rm = TRUE),#avg ratings a user gives
      rating_duration = max(rating_year) - min(rating_year),#duration users has been rating movies for a movie connoisseur variable
      recent_rating_count = sum(rating_year >= (max(rating_year) - 1)),#recent count for a movie connoisseur variable
      last_rating_date = max(as_datetime(timestamp)),#date of last rating for recentcy variable
      avg_ratings_per_year = n() / (max(rating_year) - min(rating_year) + 1),#average ratings users give a year
      rating_std_dev = sd(rating, na.rm = TRUE),#standard deviation of user ratings given a year
      trend = ifelse(n() > 1, coef(lm(rating ~ rating_year))[2], 0)#trend of user ratings a year
    ) %>%
    ungroup() %>%
    mutate(trend = ifelse(is.na(trend), 0, trend)) # Replace NA trends with 0 
  
  # Step 3: Movie-specific features
  movie_features <- data %>%
    group_by(movieId) %>%
    summarise(
      movie_rating_count = n(),#count of movie ratings
      movie_avg_rating = mean(rating, na.rm = TRUE),#average movie ratings 
      movie_rating_std_dev = sd(rating, na.rm = TRUE)#standard deviation of movie ratings
    ) %>%
    ungroup() %>%
    mutate(movie_rating_std_dev = ifelse(is.na(movie_rating_std_dev), 0, movie_rating_std_dev)) # Remove NAs 
  
  # Step 4: Combine all features into the main dataset
  data <- data %>%
    left_join(user_features, by = "userId") %>%
    mutate(
      user_movie_diff = rating - user_avg_rating,
      days_since_last_rating = as.numeric(difftime(as_datetime(timestamp), last_rating_date, units = "days"))
    ) %>%
    left_join(movie_features, by = "movieId") %>%
    mutate(years_since_release = rating_year - movie_year)
  
  # Rename columns to replace hyphens with underscores
  colnames(data) <- gsub("-", "_", colnames(data))
  
  return(data)
}
```

Now that the feature_engineering function has been created, it needs to be applied to the edx dataset, generating a new dataset (edx_clean) enriched with the engineered features and cleaned of NAs.

```{r apply feature_engineering function, echo=TRUE}
# Apply the feature engineering function to the edx dataset
edx_clean <- feature_engineering(edx)
```

## Feature Enginneering Validation
Now that the feature engineering process has been applied, the following code validates that the feature_engineering function worked appropriately with the edx dataset and is clean. 


```{r check-missing-values, echo=TRUE}
# Check for missing values in key features
cat("Missing values check for main features:\n")
summary(edx_clean %>% select(user_rating_count, user_avg_rating, rating_duration, recent_rating_count, movie_rating_count, movie_avg_rating))
```

The summary above suggests no missing values in key features, ensuring they are ready for modeling.

The histograms below reveal a normal distributions for average ratings and skewed distributions for user and movie-specific rating counts.

```{r check-feature distribution, echo=TRUE}
# Visualize distributions of user and movie-specific features
edx_clean %>%
  select(user_rating_count, user_avg_rating, movie_rating_count, movie_avg_rating) %>%
  pivot_longer(cols = everything(), names_to = "Feature", values_to = "Value") %>%
  ggplot(aes(x = Value)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  facet_wrap(~ Feature, scales = "free_x") +
  labs(title = "Distributions of Key Features", x = "Value", y = "Frequency") +
  theme_minimal()
```

The following checks confirms all columns have meaningful values, and the processed dataset now includes 26 columns, instead of the original 6. The first table below confirms the development if 20 new features, in additional to the 6 original columns, and the absence of missing values. However, there are several columns that show all 0s, so table two confirm these columns aren't empty. With 0 values numbering from 3 to 9 million, effective feature development is supported thus far.

```{r check for non-zero values, echo=TRUE}
# Check the head of all features and validate non-zero values
head(edx_clean)

# Check for columns with non-zero values
non_zero_summary <- edx_clean %>%
  summarise(across(everything(), ~ sum(. != 0, na.rm = TRUE)))
print("Number of non-zero values per column:")
print(non_zero_summary)
```

The histogram below reveals most trends are centered at 0, with some positive and negative deviations indicating variability in user behavior. This variability could support our prediction model.

```{r visualize rating distribution, echo=TRUE}
# Visualize the distribution of user rating trends
ggplot(edx_clean, aes(x = trend)) +
  geom_histogram(bins = 30, fill = "green", alpha = 0.7) +
  labs(title = "Distribution of User Rating Trends", x = "Trend", y = "Frequency") +
  theme_minimal()
```

The decade distribution confirms the feature captures historical trends, with most ratings for movies from the 1980s, 1990s, and 2000s.

```{r validate deacde feature, echo=TRUE}
# View and visualize decade distribution
cat("\nDecade distribution in dataset:\n")
decade_distribution <- table(edx_clean$decade)
print(decade_distribution)

ggplot(as.data.frame(decade_distribution), aes(x = Var1, y = Freq)) +
  geom_bar(stat = "identity", fill = "purple", alpha = 0.7) +
  labs(title = "Decade Distribution of Movies", x = "Decade", y = "Count") +
  theme_minimal()
```

The following code shows the dataset now contains 9,000,055 rows and 26 columns, providing a robust foundation for machine learning.

```{r final check of deminsions, echo=TRUE}
# Check the final dataset dimensions
cat("\nFinal dataset dimensions:\n")
cat("Rows:", nrow(edx_clean), "Columns:", ncol(edx_clean), "\n")
```

## Modeling
The fun part begins! The dataset has been cleaned and enhanced with engineered features, maximizing the utility of the 9 million rows of data within the edx_clean portion of the MovieLens dataset.

Next, the dataset is split again to serve a similar purpose as before: testing the model’s accuracy on an independent dataset. This ensures its ability to predict ratings for new data. The split divides the dataset into a train_set for model training and a validation_set for evaluation.

```{r split data into Training and Validation_set, echo=TRUE, warning=FALSE}
# Partition the edx_clean data into training and validation sets 
set.seed(1, sample.kind = "Rounding")  # set seed 1 for reproducability
train_index <- createDataPartition(y = edx_clean$rating, times = 1, p = 0.8, list = FALSE)

# Create training and validation sets
train_set <- edx_clean[train_index, ]
validation_set <- edx_clean[-train_index, ]

# Check dimensions of the training and validation sets
cat("Training set dimensions: Rows:", nrow(train_set), "Columns:", ncol(train_set), "\n")
cat("Validation set dimensions: Rows:", nrow(validation_set), "Columns:", ncol(validation_set), "\n")
```

As seen above, our first model will be trained on 7,200,045 rows of data across 26 variables and later it will be evaluated on 1,800,010 rows and 26 columns.

### Selecting Regression For Modeling 

The methodology chosen for this task is regression, where the model treats movie ratings as a continuous variable for prediction. Given the extensive amount of data available for training and the 10 possible rating levels (from 0.5 to 5.0 in 0.5 increments), regression is expected to yield highly accurate predictions. However, this comes at the cost of precision, as the model may predict ratings at arbitrary decimal points rather than adhering strictly to the 0.5-point increments that a classification approach would enforce.

Gaussian regression is particularly well-suited for this task, as it is designed to model continuous target variables effectively. To prepare the dataset for this methodology, all predictors must be numeric. This process involves removing nonessential non-numeric variables (e.g., movie titles, as the movieId already serves this purpose) and encoding essential non-numeric variables (e.g., genres, rating_day_of_week) into a numeric format.

```{r encode essential non-numeric variables as numeric, echo=TRUE,message=FALSE,warning=FALSE}
# Identify numeric columns and essential categorical columns
essential_columns <- c("genres", "last_rating_date", "rating_day_of_week", "decade")
selected_columns <- union(names(train_set)[sapply(train_set, is.numeric)], essential_columns)

# Subset the training and validation datasets
train_set_numeric <- train_set[, intersect(colnames(train_set), selected_columns)]
validation_set_numeric <- validation_set[, intersect(colnames(validation_set), selected_columns)]

# Log the column names and dimensions
print(colnames(train_set_numeric))
cat("Dimensions of train_set_numeric: Rows:", nrow(train_set_numeric), "Columns:", ncol(train_set_numeric), "\n")

print(colnames(validation_set_numeric))
cat("Dimensions of validation_set_numeric: Rows:", nrow(validation_set_numeric), "Columns:", ncol(validation_set_numeric), "\n")
```

The above show that in both the training set and the validation set there is no longer 26 variables, but 25 (removed title), all of which are now numeric. Later on in the process, this will need to be done in the Final Holdout test set as well.

The next step involves setting up the predictor matrix (x) and the target variable (y). The target variable is the movie rating, while the predictor matrix consists of all other numeric features. This setup ensures the data is ready for training the regression model.

```{r prepare predictors (x) and target variable (y), echo=TRUE, message=FALSE, warning=FALSE }
# Prepare predictor matrix (x) and target variable (y) (This might take a few minutes)
x <- as.matrix(train_set_numeric[, colnames(train_set_numeric) != "rating"])
y <- train_set_numeric$rating
```

With the dataset prepared and features engineered, the next step involves training an elastic net regression model using the Gaussian family. Elastic net regression is a robust technique that combines ridge and lasso regression for regularization, addressing overfitting while retaining the most relevant features. Ridge regression applies a penalty proportional to the squared magnitude of coefficients, effectively distributing the impact across correlated variables. In contrast, lasso regression adds a penalty proportional to the absolute value of coefficients, shrinking some coefficients to zero and excluding less relevant predictors. Elastic net leverages both approaches, balancing them with the parameter alpha. For this model, alpha = 0.5 was chosen to achieve an optimal balance between these two regularization techniques.

Cross-validation is employed to identify the optimal regularization parameter AKA lambda (which minimizes prediction error while maintaining feature interpretability).

```{r Perform elastic net regression (this will take a few minutes), echo= TRUE, error = FALSE, message=FALSE, warning=FALSE}
# Elastic net regression with Gaussian family for continuous target
# Set alpha to 0.5 for a balance between ridge and lasso regression
elastic_net_model <- cv.glmnet(x, y, alpha = 0.5, family = "gaussian")
```

Now that the net has been cast, the approach is visualized below.

```{r Visualize Lambda vs RMSE, echo= TRUE, error = FALSE, message=FALSE, warning=FALSE}
# Calculate RMSE from cross-validation default of MSE
rmse_values <- sqrt(elastic_net_model$cvm)

# Plot RMSE vs. log(lambda)
plot(
  log(elastic_net_model$lambda), rmse_values,
  type = "b", 
  xlab = "log(Lambda)",
  ylab = "Root Mean Squared Error (RMSE)",
  main = "RMSE vs. log(Lambda)"
)

# Add vertical lines for lambda.min and lambda.1se
abline(v = log(elastic_net_model$lambda.min), col = "blue", lty = 2, lwd = 2)
abline(v = log(elastic_net_model$lambda.1se), col = "red", lty = 2, lwd = 2)

# Add a legend for the lines
legend("topright", legend = c("lambda.min", "lambda.1se"),
       col = c("blue", "red"), lty = 2, lwd = 2)

# Extract RMSE values for lambda.min and lambda.1se
lambda_min_rmse <- rmse_values[which(elastic_net_model$lambda == elastic_net_model$lambda.min)]
lambda_1se_rmse <- rmse_values[which(elastic_net_model$lambda == elastic_net_model$lambda.1se)]

# Print results
cat("RMSE for lambda.min:", lambda_min_rmse, "\n")
cat("RMSE for lambda.1se:", lambda_1se_rmse, "\n")

```

The cross-validation plot generated by plot(elastic_net_model) illustrates the relationship between lambda and the model’s predictive performance. The x-axis represents lambda), the logarithm of the regularization parameter. Smaller lambda values include more features in the model, while larger lambda values exclude less significant predictors by shrinking their coefficients to zero. The y-axis shows the root mean squared error (RMSE), the model’s error metric during cross-validation. The curve depicts how RMSE varies across different lambda values, while the top axis indicates the number of features included in the model at each lambda value.

The leftmost part of the plot (smallest lambda) represents the model with the least regularization and the largest number of features. Conversely, the rightmost part (largest lambda represents the model with the most regularization and the fewest features. As shown by the blue line (which is obscured by the red line), the minimum RMSE point identifies the lambda value that achieves the lowest prediction error, referred to as lambda_min. Some approaches use 1se, but since both values are .0315 the lambda.min value will be used to build the final model.

The optimal lambda is determined by the lambda_min value from cross-validation. This parameter minimizes prediction error while maintaining model simplicity by excluding irrelevant features.

```{r Get Best Lambda, echo=TRUE,message=FALSE, warning=FALSE}
# Identify the lambda corresponding to the minimum RMSE
best_lambda_rmse <- elastic_net_model$lambda[which.min(rmse_values)]
print(paste("Best lambda (based on RMSE):", best_lambda_rmse))

# Fit the model with the best lambda based on RMSE
final_model <- glmnet(x, y, alpha = 0.5, lambda = best_lambda_rmse)

# Print coefficients of the final model
print(coef(final_model))
```

The optimal lambda minimizes cross-validated error, and the coefficients of the final model highlight the features retained after regularization. Larger coefficients indicate greater importance in predicting movie ratings. Key predictors such as user_movie_diff, user_avg_rating, and movie_avg_rating emerge as the most influential, which aligns with the objective of predicting ratings based on user and movie characteristics.

This finalized model will now be evaluated on the validation set to assess its predictive performance on unseen data.

```{r , Use model to predict values in validation dataset, echo=TRUE,message=FALSE, warning=FALSE}
# Prepare the predictor matrix (x_val) and target variable (y_val)
x_val <- as.matrix(validation_set_numeric[, colnames(validation_set_numeric) != "rating"])
y_val <- validation_set_numeric$rating


# Refit the final model using the RMSE-optimized lambda
final_model_rmse <- glmnet(x, y, alpha = 0.5, lambda = best_lambda_rmse)

# Predict ratings for the validation set using the RMSE-optimized final model
predictions <- predict(final_model_rmse, newx = x_val)

# Calculate RMSE on the validation set
rmse_validation <- sqrt(mean((predictions - y_val)^2))
cat("RMSE on Validation Set:", rmse_validation, "\n")
```

The RMSE on the validation set is 0.0315, indicating that the model’s predictions deviate from the actual ratings by an average of only 0.03, a strong performance given that movie ratings are provided in increments of 0.5.

Below is a visualization of the residuals of the predicted ratings:
```{r visualize predicted vs actual ratings, echo=TRUE}
#check predicted vs actual
residuals <- y_val - predictions
hist(residuals, breaks = 50, main = "Residuals Distribution", xlab = "Residuals")
```

The residuals, representing the difference between actual and predicted ratings, are tightly clustered between -0.05 and +0.05, indicating that the model predicts movie ratings with high accuracy and minimal error. This narrow range suggests the model effectively captures the underlying patterns in the data, with no significant bias or overestimation.

With this strong result on the validation set, it's time to test the developed model on the independent final_hold set.

# Results
In this section, the trained model is evaluated on the final holdout set to assess its performance on completely unseen data. The process involves applying feature engineering, aligning predictors, and calculating the Root Mean Squared Error (RMSE), which serves as the primary metric for model accuracy.

## Preparing the Final Holdout Set

To ensure consistency, feature engineering is applied to the final holdout set. Numeric and essential categorical columns are retained, while unnecessary columns like title are excluded. The processed dataset is then prepared for prediction.

```{r prepare final holdout test, echo =TRUE, message = FALSE, warning = FALSE}
# Apply the feature engineering function to the final holdout set
final_holdout_clean <- feature_engineering(final_holdout_test)

# Ensure only numeric columns and essential categorical columns are included
essential_columns <- c("genres", "last_rating_date", "rating_day_of_week", "decade")
selected_columns <- union(names(final_holdout_clean)[sapply(final_holdout_clean, is.numeric)], essential_columns)
final_holdout_numeric <- final_holdout_clean[, intersect(colnames(final_holdout_clean), selected_columns)]

# Remove unnecessary columns like 'title'
final_holdout_numeric <- final_holdout_numeric[, colnames(train_set_numeric)]

# Verify dimensions and alignment
cat("Number of columns in final holdout numeric dataset:", ncol(final_holdout_numeric), "\n")
```

The processed final holdout dataset ensures compatibility with the trained elastic net regression model, maintaining alignment in the number of predictors (25).

The predictor matrix and target variable are prepared from the processed final holdout set. Predictions are generated using the trained model, and RMSE is calculated to quantify the model’s accuracy.

```{r Predict on the holdout Set, echo = TRUE, message = FALSE, warning = FALSE}
# Prepare the predictor matrix and target variable
x_holdout <- as.matrix(final_holdout_numeric[, colnames(final_holdout_numeric) != "rating"])
y_holdout <- final_holdout_numeric$rating

# Ensure dimensions match before predicting
if (ncol(x_holdout) == length(coef(final_model)) - 1) {
  # Predict ratings for the final holdout set using the trained model
  predictions_holdout <- predict(final_model, newx = x_holdout)
  
  # Calculate RMSE on the final holdout set
  rmse_holdout <- sqrt(mean((predictions_holdout - y_holdout)^2))
  cat("RMSE on Final Holdout Set:", rmse_holdout, "\n")
}
```
## Output
The RMSE calculated on the final holdout was ~.03262. This is a negligible increase from the models performance on the validation set .03147. As a result, we can conclude this model provides a robust measure of how accurately the model predicts movie ratings for unseen data.

Like the grey residuals histogram generated for the validation_set performance, the predictions fell within +.05 and - .05 for the majoirty of predictions. This results shows the strength of the model by way of predictions being between 1 rating level.

```{r Visualize Model Performance on Final Holdout Set, echo=TRUE, warning=FALSE}
# Calculate residuals for the final holdout set
residuals_holdout <- y_holdout - predictions_holdout

# Visualize the residuals
hist(
  residuals_holdout, 
  breaks = 50, 
  main = "Residuals Distribution for Final Holdout Set", 
  xlab = "Residuals",
  col = "skyblue",
  border = "white"
)
```

## Top 10 Largest Errors
The table below shows that the largest mistakes were .13 off and these mistakes don't appear to be in any particular genre or decade, suggesting no absolutely necessary adjustments are needed.

```{r Table of Top 10 Largest Errors, echo=TRUE}
#Calculate residuals and absolute residuals
residuals_holdout <- y_holdout - predictions_holdout
absolute_residuals <- abs(residuals_holdout)

# Find the top 10 largest errors
largest_errors <- order(absolute_residuals, decreasing = TRUE)[1:10]

# Create a table with movie details and error metrics for the largest errors
largest_errors_table <- data.frame(
  final_holdout_clean[largest_errors, c("movieId", "title", "genres")], # Movie details
  Actual = y_holdout[largest_errors],                                  # Actual ratings
  Predicted = predictions_holdout[largest_errors],                    # Predicted ratings
  Residual = residuals_holdout[largest_errors]                        # Residuals
)

# Print the table
cat("Details of Movies with the Largest Prediction Errors:\n")
print(largest_errors_table)
```


# Conslusion
This report has loaded, cleaned, and developed features for a training set of the MovieLens 10mmillion dataset. Afterward, features were engineering to increase the power of the data available. Next, a machine learning algorithm was trained using Gaussian regression with an elastic net technique. Once trained, the model produced an RMSE of ~.0315 on the validation set. Finally, the model was used to predict movies ratings on the Final Holdout set and produced an RMSE of ~.0326. This results confirms that the model effectively predicts movie ratings within a half star out of 5 stars. Therefore, this model is an effective tool for predicting movielens movie ratings. 

## Limtations
Despite extensive preprocessing, the feature engineering pipeline heavily relies on the absence of missing values in the dataset. If applied to new, uncleaned datasets, the feature creation function may fail due to NA values or unexpected data formats. Enhancing robustness through NA handling and input validation would improve the model’s scalability.

The Gaussian regression model allowed for continuous predictions, providing a higher degree of precision in estimating movie ratings. However, this approach diverges from the discrete nature of the actual rating system, which operates in 0.5-star increments. While the model’s continuous predictions (e.g., 3.6 stars) may offer more granularity, they are not valid within the constraints of the original system and may require rounding or mapping to the closest valid rating for practical applications.

The dataset’s historical nature introduces data imbalance, particularly for older movies with fewer ratings. This imbalance may reduce prediction accuracy for specific subsets, such as niche or vintage films. Moreover, temporal effects, such as seasonal spikes in viewership (e.g., horror films during Halloween), were not explicitly modeled, potentially leaving valuable information untapped.

## Future Direction
To address the limitation of invalid rating predictions, future iterations could explore classification models. These would predict discrete rating categories (e.g., 0.5-star increments) rather than continuous values. Techniques such as ordinal regression or multi-class classification with softmax outputs could be effective for this purpose

The model currently incorporates basic user-specific features such as average ratings and rating trends. Expanding this feature set to include interactions with genres, time of year (e.g., holidays), or platform activity patterns (e.g., binge-watching behavior) could improve predictive performance and provide deeper insights into user preferences.

Incorporating temporal data, such as the release year or month, and modeling seasonal patterns (e.g., holiday-related spikes) could significantly improve predictions. This direction may involve feature engineering to identify periods when movies gain or lose popularity or sentiment shifts over time.

Feature engineering can be further expanded to include embeddings for movies, genres, or users using techniques like collaborative filtering or deep learning approaches (e.g., matrix factorization or neural networks). These methods could uncover latent factors influencing ratings, yielding a more robust and scalable prediction system.

# Citations
Durrani, A. (2024, August 15). Top streaming statistics in 2024. Forbes. https://www.forbes.com/home-improvement/internet/streaming-stats/ 

F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. https://doi.org/10.1145/2827872

Jesse Vig, Shilad Sen, and John Riedl. 2012. The Tag Genome: Encoding Community Knowledge to Support Novel Interaction. ACM Trans. Interact. Intell. Syst. 2, 3: 13:1–13:44. https://doi.org/10.1145/2362394.2362395

Link to MovieLens dataset 10mil: https://grouplens.org/datasets/movielens/10m/


